library(twitteR)
library(RCurl)
library(XML)
library(ROAuth)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL = "http://api.twitter.com/oauth/access_token"
authURL = "http://api.twitter.com/oauth/authorize"
consumerKey = "qbnVaHtXMaz40OiWlA"
consumerSecret = "XtNtyVRQJPR3xRq1cdjkwmsJA2zyJ94TepZRiAWz4"
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
?userTimeline
#The next command provides a URL which you will need to copy and paste into your favourite browser
#Assuming you are logged into Twitter you will then be provided a PIN number to type into the R command line
Cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl") );
registerTwitterOAuth(Cred)
rdmTweets <- userTimeline("davefontenot", n=1500)
(nDocs <- length(rdmTweets))
rdmTweets[11:15]
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(rdmTweets[[i]]$getText(), width=73))
}
# convert tweets to a data frame
df <- do.call("rbind", lapply(rdmTweets, as.data.frame))
dim(df)
install.packages("tm")
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
idx <- which(myStopwords %in% c("r", "big"))
myStopwords <- myStopwords[-idx]
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
idx <- which(myStopwords %in% c("r", "big"))
myStopwords <- myStopwords[-idx]
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf)))
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
library(ggplot2)
qplot(names(termFrequency), termFrequency, geom="bar") + coord_flip()
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
#######ggplot Example
qplot(names(termFrequency), termFrequency, geom="bar") + coord_flip()
library(wordcloud)
m <- as.matrix(myTdm)
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf)))
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
termFrequency <- rowSums(as.matrix(myTdm))
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
idx <- which(myStopwords %in% c("r", "big"))
myStopwords <- myStopwords[-idx]
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
library(SnowballC)
installed.packages(SnowballC)
installed.packages("SnowballC")
install.packages("SnowballC")
library(SnowballC)
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf)))
dim(df)
df
library(wordcloud)
myTdm
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf)))
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myCorpus
myStopwords
idx
myStopwords
list myStopwords
mode(myStopwords)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
myStopwords
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf)))
? TermDocumentMatrix
wordLength
? wordLength
?? wordLength
myTdm <- TermDocumentMatrix(myCorpus) #, control = list(wordLengths=c(1,Inf))
######word cloud
library(wordcloud)
m <- as.matrix(myTdm)
m <- as.matrix(myCorpus)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
rm x
x
mode(x)
m
mode(m)
attr(m , numeric)
attr(m , "numeric")
attributes(m)
wordFreq <- sort(rowSums(m), decreasing=TRUE)
mode(m)
list(m)
m[1]
m[[1]]
library(SnowballC)
m[[1]]
loadNamespace(m[1])
loadNamespace(m[[1]])
loadNamespace("SnowballC")
m[[1]]
set.seed(375) # to make it reproducible
wordFreq <- sort(m, decreasing = TRUW)
wordFreq <- sort(m, decreasing = TRUE)
wordcloud(words=names(m), freq=m, min.freq=3, random.order=F,
colors=grayLevels)
wordcloud(words=names(m), min.freq=3, random.order=F,
colors=grayLevels)
wordcloud(words=names(m), min.freq=3, random.order=F,
)
library(slam)
myTdm <- TermDocumentMatrix(myCorpus) #, control = list(wordLengths=c(1,Inf))
myTdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = TRUE)) #, control = list(wordLengths=c(1,Inf))
nDocs
fix(rdmTweets)
fix(rdmTweets)
rdmTweets <- userTimeline("davefontenot", n=120)
(nDocs <- length(rdmTweets))
rdmTweets[11:15]
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(rdmTweets[[i]]$getText(), width=73))
}
# convert tweets to a data frame
df <- do.call("rbind", lapply(rdmTweets, as.data.frame))
dim(df)
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
idx <- which(myStopwords %in% c("r", "big"))
myStopwords <- myStopwords[-idx]
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myStopwords
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
idx <- which(myStopwords %in% c("r", "big"))
myStopwords <- myStopwords[-idx]
myStopwords
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
myStopwords
myCorpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myTdm <- TermDocumentMatrix(myCorpus, control = list(removePunctuation = TRUE,
stopwords = TRUE)) #, control = list(wordLengths=c(1,Inf))
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
#######ggplot Example
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf))) #
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
library(ggplot2)
qplot(names(termFrequency), termFrequency, geom="bar") + coord_flip()
######word cloud
library(wordcloud)
m <- as.matrix(myTdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
# word cloud
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F,
colors=grayLevels)
rdmTweets <- userTimeline("davefontenot", n=1500)
(nDocs <- length(rdmTweets))
rdmTweets[11:15]
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(rdmTweets[[i]]$getText(), width=73))
}
# convert tweets to a data frame
df <- do.call("rbind", lapply(rdmTweets, as.data.frame))
dim(df)
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
# add two extra stop words: "available" and "via"
idx <- which(myStopwords %in% c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width
for (i in 11:15) {
cat(paste("[[", i, "]] ", sep=""))
writeLines(strwrap(myCorpus[[i]], width=73))
}
myTdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf))) #
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
#######ggplot Example
library(ggplot2)
qplot(names(termFrequency), termFrequency, geom="bar") + coord_flip()
library(wordcloud)
m <- as.matrix(myTdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
# word cloud
set.seed(375) # to make it reproducible
grayLevels <- gray( (wordFreq+10) / (max(wordFreq)+10) )
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=3, random.order=F,
colors=grayLevels)
installed.packages("rggobi")
install.packages("rggobi", dep =T)
install.packages("ggobi")
install.packages("ggobi", dept =T)
install.packages("rggobi", dep =T)
library(ggobi)
install.packages("DescribeDisplay", dep=TRUE)
q()
